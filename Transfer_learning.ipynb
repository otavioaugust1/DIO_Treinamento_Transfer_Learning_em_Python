{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importação da biblioteca\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../DIO_transfer_learning-AI_de_reconhecimento/img/cats', '../DIO_transfer_learning-AI_de_reconhecimento/img/dogs']\n"
     ]
    }
   ],
   "source": [
    "path = '../DIO_transfer_learning-AI_de_reconhecimento/img/'\n",
    "root = path\n",
    "exclude = ['.ipynb_checkpoints']\n",
    "train_split, val_split = 0.7, 0.15\n",
    "\n",
    "categories = [x[0] for x in os.walk(root) if x[0]][1:]\n",
    "categories = [c for c in categories if c not in [os.path.join(root, e) for e in exclude]]\n",
    "\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função auxiliar para carregar a imagem e devolvê-la e inserir o vetor\n",
    "def get_image(path):\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return img, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.preprocessing.image' has no attribute 'load_img'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\otavi\\GitHub\\DIO_transfer_learning-AI_de_reconhecimento\\Transfer_learning.ipynb Célula: 4\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/otavi/GitHub/DIO_transfer_learning-AI_de_reconhecimento/Transfer_learning.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     images \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dp, f) \u001b[39mfor\u001b[39;00m dp, dn, filenames \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/otavi/GitHub/DIO_transfer_learning-AI_de_reconhecimento/Transfer_learning.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m               \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mwalk(category) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m filenames \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/otavi/GitHub/DIO_transfer_learning-AI_de_reconhecimento/Transfer_learning.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m               \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(f)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mlower() \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m.jpeg\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/otavi/GitHub/DIO_transfer_learning-AI_de_reconhecimento/Transfer_learning.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m img_path \u001b[39min\u001b[39;00m images:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/otavi/GitHub/DIO_transfer_learning-AI_de_reconhecimento/Transfer_learning.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         img, x \u001b[39m=\u001b[39m get_image(img_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/otavi/GitHub/DIO_transfer_learning-AI_de_reconhecimento/Transfer_learning.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         data\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m:np\u001b[39m.\u001b[39marray(x[\u001b[39m0\u001b[39m]), \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m:c})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/otavi/GitHub/DIO_transfer_learning-AI_de_reconhecimento/Transfer_learning.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m num_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(categories)\n",
      "\u001b[1;32mc:\\Users\\otavi\\GitHub\\DIO_transfer_learning-AI_de_reconhecimento\\Transfer_learning.ipynb Célula: 4\u001b[0m in \u001b[0;36mget_image\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/otavi/GitHub/DIO_transfer_learning-AI_de_reconhecimento/Transfer_learning.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_image\u001b[39m(path):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/otavi/GitHub/DIO_transfer_learning-AI_de_reconhecimento/Transfer_learning.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     img \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39;49mload_img(path, target_size\u001b[39m=\u001b[39m(\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/otavi/GitHub/DIO_transfer_learning-AI_de_reconhecimento/Transfer_learning.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     x \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mimg_to_array(img)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/otavi/GitHub/DIO_transfer_learning-AI_de_reconhecimento/Transfer_learning.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(x, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.preprocessing.image' has no attribute 'load_img'"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for c, category in enumerate(categories):\n",
    "    images = [os.path.join(dp, f) for dp, dn, filenames \n",
    "              in os.walk(category) for f in filenames \n",
    "              if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
    "    for img_path in images:\n",
    "        img, x = get_image(img_path)\n",
    "        data.append({'x':np.array(x[0]), 'y':c})\n",
    "\n",
    "\n",
    "num_classes = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx_val = int(train_split * len(data))\n",
    "idx_test = int((train_split + val_split) * len(data))\n",
    "train = data[:idx_val]\n",
    "val = data[idx_val:idx_test]\n",
    "test = data[idx_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, y_train = np.array([t[\"x\"] for t in train]), [t[\"y\"] for t in train]\n",
    "x_val, y_val = np.array([t[\"x\"] for t in val]), [t[\"y\"] for t in val]\n",
    "x_test, y_test = np.array([t[\"x\"] for t in test]), [t[\"y\"] for t in test]\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_val = x_val.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# converte rótulos em vetores one-hot\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.np_utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summario\n",
    "print(\"finished loading %d images from %d categories\"%(len(data), num_classes))\n",
    "print(\"train / validation / test split: %d, %d, %d\"%(len(x_train), len(x_val), len(x_test)))\n",
    "print(\"training data shape: \", x_train.shape)\n",
    "print(\"training labels shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
    "idx = [int(len(images) * random.random()) for i in range(8)]\n",
    "imgs = [image.load_img(images[i], target_size=(224, 224)) for i in idx]\n",
    "concat_image = np.concatenate([np.asarray(img) for img in imgs], axis=1)\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.imshow(concat_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construa a rede (AI)\n",
    "model = Sequential()\n",
    "print(\"Input dimensions: \",x_train.shape[1:])\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compila o modelo para usar a função de perda de entropia cruzada categórica e o otimizador adadelta\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,4))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(history.history[\"val_loss\"])\n",
    "ax.set_title(\"validation loss\")\n",
    "ax.set_xlabel(\"epochs\")\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(history.history[\"val_accuracy\"])\n",
    "ax2.set_title(\"validation accuracy\")\n",
    "ax2.set_xlabel(\"epochs\")\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=True)\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a reference to VGG's input layer\n",
    "inp = vgg.input\n",
    "\n",
    "# make a new softmax layer with num_classes neurons\n",
    "new_classification_layer = Dense(num_classes, activation='softmax')\n",
    "\n",
    "# connect our new layer to the second to last layer in VGG, and make a reference to it\n",
    "out = new_classification_layer(vgg.layers[-2].output)\n",
    "\n",
    "# create a new network between inp and out\n",
    "model_new = Model(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all layers untrainable by freezing weights (except for last layer)\n",
    "for l, layer in enumerate(model_new.layers[:-1]):\n",
    "    layer.trainable = False\n",
    "\n",
    "# ensure the last layer is trainable/not frozen\n",
    "for l, layer in enumerate(model_new.layers[-1:]):\n",
    "    layer.trainable = True\n",
    "\n",
    "model_new.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model_new.fit(x_train, y_train, \n",
    "                         batch_size=128, \n",
    "                         epochs=100, \n",
    "                         validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,4))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(history.history[\"val_loss\"])\n",
    "ax.plot(history2.history[\"val_loss\"])\n",
    "ax.set_title(\"validation loss\")\n",
    "ax.set_xlabel(\"epochs\")\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(history.history[\"accuracy\"])\n",
    "ax2.plot(history2.history[\"accuracy\"])\n",
    "ax2.set_title(\"validation accuracy\")\n",
    "ax2.set_xlabel(\"epochs\")\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model_new.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7f8c097c8b77234b1554043a16e456bc3013eb7e1454957444d1d61c725966c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
